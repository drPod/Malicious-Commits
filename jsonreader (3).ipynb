{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d671e05d-de62-48a5-8dab-7d1c0a8465d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\epguo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#package imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402c8ed9-d276-4b81-a717-0002a7ec353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6048cc3-a34c-418d-88d7-de275faa12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#security related keywords\n",
    "security_words = ['buffer', 'corrupt', 'boundary', 'valid', 'invalid','bypass','neutral','restrict',\n",
    "                  'pathname','verifi','verif','query','request','author', 'authent','command','permission',\n",
    "                  'password','key','deseri','encrypt','decrypt','credenti','passkey','depend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e3cf49-903f-4256-82fa-364c6a617b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_fxns = ['malloc','calloc','memcpy','free','realloc','alloc','memchr','memcmp','memmove','memset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b25a572-f8c2-4766-b4bf-b6db479fa43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this block \n",
    "json_file = 'CVE-1999-0199.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32af4daa-3497-4703-a470-bffaea410052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "file_changes = data['file_changes']\n",
    "tokens_added = 0\n",
    "tokens_removed = 0\n",
    "mem_words_present = []\n",
    "sec_words_present = []\n",
    "for i in file_changes:\n",
    "    for j in file_changes[i]:\n",
    "        if j != 'used_context_lines':\n",
    "            k = 0\n",
    "            while k < len(file_changes[i][j]):\n",
    "                line = file_changes[i][j][k]\n",
    "                #print(line)\n",
    "                mem_words_present = mem_words_present + [word for word in memory_fxns if word in line]\n",
    "                if( line != \"\" and line[0] == '-'):\n",
    "                    tokens_removed += len(word_tokenize(line))\n",
    "                if( line != \"\" and line[0] == '+'):\n",
    "                    tokens_added += len(word_tokenize(line))\n",
    "                k += 1\n",
    "\n",
    "commit_msg = data['commit_metadata']['message'].split()\n",
    "commit_msg_stemmed = [stemmer.stem(word) for word in commit_msg]\n",
    "commit_msg_stemmed = ' '.join(commit_msg_stemmed)\n",
    "\n",
    "sec_words_present = sec_words_present + [word for word in security_words if word in commit_msg_stemmed]\n",
    "    \n",
    "\n",
    "# print(tokens_added)\n",
    "# print(tokens_removed)\n",
    "# print(mem_words_present)\n",
    "# print(sec_words_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f46a98-9167-426c-ac0f-181301dc728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        file_changes = data['file_changes']\n",
    "        tokens_added = 0\n",
    "        tokens_removed = 0\n",
    "        mem_words_present = []\n",
    "        sec_words_present = []\n",
    "\n",
    "        #tokens added or removed with memory related fxns\n",
    "        for i in file_changes:\n",
    "            for j in file_changes[i]:\n",
    "                if j != 'used_context_lines':\n",
    "                    k = 0\n",
    "                    while k < len(file_changes[i][j]):\n",
    "                        line = file_changes[i][j][k]\n",
    "                        mem_words_present = mem_words_present + [word for word in memory_fxns if word in line]\n",
    "                        if( line != \"\" and line[0] == '-'):\n",
    "                            tokens_removed += len(word_tokenize(line))\n",
    "                        if( line != \"\" and line[0] == '+'):\n",
    "                            tokens_added += len(word_tokenize(line))\n",
    "                        k += 1\n",
    "\n",
    "        #security related keywords in commit msg\n",
    "        commit_msg = data['commit_metadata']['message'].split()\n",
    "        commit_msg_stemmed = [stemmer.stem(word) for word in commit_msg]\n",
    "        commit_msg_stemmed = ' '.join(commit_msg_stemmed)\n",
    "\n",
    "        sec_words_present = sec_words_present + [word for word in security_words if word in commit_msg_stemmed]\n",
    "            \n",
    "        #metadata\n",
    "        metadata = data['commit_metadata']\n",
    "        #basic counts\n",
    "        author_length = len(metadata['author'])\n",
    "        email_length = len(metadata['author_email'])\n",
    "        msg_length = len(metadata['message'])\n",
    "        #tokens\n",
    "        # sec_words_present = [word for word in security_words if word in metadata['message'].lower()]\n",
    "        # token_msg = word_tokenize(metadata['message'])\n",
    "        # tokens = len(token_msg)\n",
    "        df.loc[len(df.index)] = [author_length, email_length, msg_length, sec_words_present, mem_words_present, tokens_added, tokens_removed]\n",
    "    #display(df)          \n",
    "    df.to_csv(csv_file, index = False)\n",
    "#test run\n",
    "#update_csv('jsonfolder/CVE-1999-0199.json', 'character_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1ae98-d3e6-48a5-af10-6e283c36a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = 'commit_metadata'\n",
    "csv_file = 'character_counts.csv'\n",
    "df = pd.DataFrame(columns = ['Length of author name', 'Length of author email', 'Length of commit message', 'Security related keywords in message', 'Memory related functions in code changes', 'Tokens added in commit', 'Tokens removed in commit'])\n",
    "for file in os.listdir(json_folder):\n",
    "    if file.endswith('.json'):\n",
    "        json_file = os.path.join(json_folder, file)\n",
    "        update_csv(json_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a84c4-f1f9-43df-bd03-8d56f9c42e9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignore this block\n",
    "# current_directory = 'commit_metadata'\n",
    "\n",
    "# files = os.listdir(current_directory)\n",
    "\n",
    "# for file in files:\n",
    "#     if file.endswith('.json'):\n",
    "#         file_path = os.path.join(current_directory, file)\n",
    "#         os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb83532-abad-4213-a934-7d1868eead11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
